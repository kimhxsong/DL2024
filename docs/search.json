[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "딥러닝 (2024)",
    "section": "",
    "text": "질문하는 방법\n\n이메일: guebin@jbnu.ac.kr\n직접방문: 자연과학대학 본관 205호\nZoom: 이메일로 미리 시간을 정할 것\n카카오톡: http://pf.kakao.com/_txeIFG/chat\n\n강의노트\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 11, 2024\n\n\n02wk-2: 딥러닝의 기초 (3) – Step1,2,4 의 변형\n\n\n최규빈 \n\n\n\n\nMar 11, 2024\n\n\n02wk-1: 딥러닝의 기초 (2) – Step1 ~ 4\n\n\n최규빈 \n\n\n\n\nMar 6, 2024\n\n\n01wk-2: 딥러닝의 기초 (1) – 회귀모형, 경사하강법\n\n\n최규빈 \n\n\n\n\nMar 4, 2024\n\n\n01wk-1: 이미지 자료 분석 (겉핥기)\n\n\n최규빈 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/02wk-1.html#a.-소설",
    "href": "posts/02wk-1.html#a.-소설",
    "title": "02wk-1: 딥러닝의 기초 (2) – Step1 ~ 4",
    "section": "A. 소설",
    "text": "A. 소설\n- 카페주인인 박혜원씨는 온도와 아이스아메리카노 판매량이 관계가 있다는 것을 알았다. 구체적으로는\n\n“온도가 높아질 수록 (=날씨가 더울수록) 아이스아메리카노의 판매량이 증가”\n\n한다는 사실을 알게 되었다. 박혜원씨는\n\n일기예보를 보고 오늘의 평균 기온을 입력하면, 오늘의 아이스아메리카노 판매량을 미리 예측할 수 있지 않을까? 그 예측량만큼 아이스아메리카노를 준비하면 장사에 도움이 되지 않을까???\n\n라는 생각을 하게 되었고 이를 위하여 아래와 같이 100개의 데이터를 모았다.\n\ntemp = [-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632]\n\n\nsales= [-8.5420, -6.5767, -5.9496, -4.4794, -4.2516, -3.1326, -4.0239, -4.1862,\n        -3.3403, -2.2027, -2.0262, -2.5619, -1.3353, -2.0466, -0.4664, -1.3513,\n        -1.6472, -0.1089, -0.3071, -0.6299, -0.0438,  0.4163,  0.4166, -0.0943,\n         0.2662,  0.4591,  0.8905,  0.8998,  0.6314,  1.3845,  0.8085,  1.2594,\n         1.1211,  1.9232,  1.0619,  1.3552,  2.1161,  1.1437,  1.6245,  1.7639,\n         1.6022,  1.7465,  0.9830,  1.7824,  2.1116,  2.8621,  2.1165,  1.5226,\n         2.5572,  2.8361,  3.3956,  2.0679,  2.8140,  3.4852,  3.6059,  2.5966,\n         2.8854,  3.9173,  3.6527,  4.1029,  4.3125,  3.4026,  3.2180,  4.5686,\n         4.3772,  4.3075,  4.4895,  4.4827,  5.3170,  5.4987,  5.4632,  6.0328,\n         5.2842,  5.0539,  5.4538,  6.0337,  5.7250,  5.7587,  6.2020,  6.5992,\n         6.4621,  6.5140,  6.6846,  7.3497,  8.0909,  7.0794,  6.8667,  7.4229,\n         7.2544,  7.1967,  9.5006,  9.0339,  7.4887,  9.0759, 11.0946, 10.3260,\n        12.2665, 13.0983, 12.5468, 13.8340]\n\n여기에서 temp는 평균기온이고, sales는 아이스아메리카노 판매량이다.1 평균기온과 판매량의 그래프를 그려보면 아래와 같다.\n1 판매량이 소수점이고 심지어 음수인것은 그냥 그려러니 하자..\nplt.plot(temp,sales,'o')"
  },
  {
    "objectID": "posts/02wk-1.html#b.-모델링",
    "href": "posts/02wk-1.html#b.-모델링",
    "title": "02wk-1: 딥러닝의 기초 (2) – Step1 ~ 4",
    "section": "B. 모델링",
    "text": "B. 모델링\n- 산점도를 살펴본 박혜원씨는 평균기온이 올라갈수록 아이스아메리카노 판매량이 “선형적”으로 증가한다는 사실을 캐치했다. 물론 약간의 오차는 있어보였다. 오차까지 고려하여 평균기온과 아이스판매량의 관계를 추정하면 아래와 같이 생각할 수 있다.\n\n아이스아메리카노 판매량 \\(\\approx\\) \\(w_0\\) \\(+\\) \\(w_1\\) \\(\\times\\) 평균기온\n\n위의 수식에서 만약에 \\(w_0\\)와 \\(w_1\\)의 값을 적절히 추정한다면, 평균기온량을 입력으로 하였을때 아이스아메리카노 판매량을 예측할 수 있을 것이다.\n- 아이스크림 판매량을 \\(y_i\\)로, 평균기온을 \\(x_i\\)로 변수화한뒤 박혜원의 수식을 좀 더 수학적으로 표현하면\n\\[y_i \\approx w_0 + w_1 x_i,\\quad i=1,2,\\dots,100\\]\n와 같이 쓸 수 있다. 오차항을 포함하여 좀 더 엄밀하게 쓰면\n\\[y_i = w_0 + w_1 x_i + \\epsilon_i,\\quad i=1,2,\\dots,100\\]\n와 같이 나타낼 수 있어보인다. 여기에서 \\(\\epsilon_i \\sim N(0,\\sigma^2)\\) 로 가정해도 무방할 듯 하다. 그런데 이를 다시 아래와 같이 표현하는 것이 가능하다.\n\\[{\\bf y}={\\bf X}{\\bf W} +\\boldsymbol{\\epsilon}\\]\n단 여기에서\n\\[{\\bf y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_n\\end{bmatrix}, \\quad {\\bf x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\dots \\\\ x_n\\end{bmatrix}, \\quad {\\bf X}=\\begin{bmatrix} {\\bf 1} & {\\bf x} \\end{bmatrix}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots \\\\ 1 & x_n\\end{bmatrix}, \\quad {\\bf W}=\\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}, \\quad \\boldsymbol{\\epsilon}= \\begin{bmatrix} \\epsilon_1 \\\\ \\dots \\\\ \\epsilon_n\\end{bmatrix}\\]\n이다."
  },
  {
    "objectID": "posts/02wk-1.html#c.-데이터를-torch.tensor로-변환",
    "href": "posts/02wk-1.html#c.-데이터를-torch.tensor로-변환",
    "title": "02wk-1: 딥러닝의 기초 (2) – Step1 ~ 4",
    "section": "C. 데이터를 torch.tensor로 변환",
    "text": "C. 데이터를 torch.tensor로 변환\n- 현재까지의 상황을 파이토치로 코딩하면 아래와 같다.\n\nx = torch.tensor(temp).reshape(-1,1)\nones = torch.ones(100).reshape(-1,1)\nX = torch.concat([ones,x],axis=1)\ny = torch.tensor(sales).reshape(-1,1)\n#W = ?? 이건 모름.. 추정해야함. \n#ϵ = ?? 이것도 모름!!"
  },
  {
    "objectID": "posts/02wk-1.html#d.-아무렇게나-추정",
    "href": "posts/02wk-1.html#d.-아무렇게나-추정",
    "title": "02wk-1: 딥러닝의 기초 (2) – Step1 ~ 4",
    "section": "D. 아무렇게나 추정",
    "text": "D. 아무렇게나 추정\n- \\({\\bf W}\\) 에 대한 추정값을 \\(\\hat{\\bf W}\\)라고 할때\n\\[\\hat{\\bf W}=\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix} =\\begin{bmatrix} -5 \\\\ 10 \\end{bmatrix}\\]\n으로 추정한 상황이라면 커피판매량의 예측값은\n\\[\\hat{\\bf y} = {\\bf X}\\hat{\\bf W}\\]\n이라고 표현할 수 있다. 이 의미는 아래의 그림에서 주황색 점선으로 커피판매량을 예측한다는 의미이다.\n\nWhat = torch.tensor([[-5.0],\n                     [10.0]])\nWhat\n\ntensor([[-5.],\n        [10.]])\n\n\n\nyhat = X@What\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat,'--')"
  },
  {
    "objectID": "posts/02wk-1.html#e.-추정의-방법",
    "href": "posts/02wk-1.html#e.-추정의-방법",
    "title": "02wk-1: 딥러닝의 기초 (2) – Step1 ~ 4",
    "section": "E. 추정의 방법",
    "text": "E. 추정의 방법\n- 방법1: 이론적으로 추론 &lt;- 회귀분석시간에 배운것\n\ntorch.linalg.inv((X.T @ X)) @ X.T @ y # 공식~\n\ntensor([[2.4459],\n        [4.0043]])\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,2.4459 + 4.0043*x,'--')\n\n\n\n\n\n\n\n\n- 방법2: 컴퓨터의 반복계산을 이용하여 추론 (손실함수도입 + 경사하강법)\n\n1단계: 아무 점선이나 그어본다..\n2단계: 1단계에서 그은 점선보다 더 좋은 점선으로 바꾼다.\n3단계: 1-2단계를 반복한다."
  },
  {
    "objectID": "posts/02wk-1.html#a.-문제셋팅-다시-복습",
    "href": "posts/02wk-1.html#a.-문제셋팅-다시-복습",
    "title": "02wk-1: 딥러닝의 기초 (2) – Step1 ~ 4",
    "section": "A. 문제셋팅 다시 복습",
    "text": "A. 문제셋팅 다시 복습\n\nx = torch.tensor(temp).reshape(-1,1)\nones = torch.ones(100).reshape(-1,1)\nX = torch.concat([ones,x],axis=1)\ny = torch.tensor(sales).reshape(-1,1)"
  },
  {
    "objectID": "posts/02wk-1.html#b.-1단계-최초의-점선",
    "href": "posts/02wk-1.html#b.-1단계-최초의-점선",
    "title": "02wk-1: 딥러닝의 기초 (2) – Step1 ~ 4",
    "section": "B. 1단계 – 최초의 점선",
    "text": "B. 1단계 – 최초의 점선\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nyhat = X@What \n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.data,'--') # 그림을 그리기 위해서 yhat의 미분꼬리표를 제거"
  },
  {
    "objectID": "posts/02wk-1.html#c.-2단계-update",
    "href": "posts/02wk-1.html#c.-2단계-update",
    "title": "02wk-1: 딥러닝의 기초 (2) – Step1 ~ 4",
    "section": "C. 2단계 – update",
    "text": "C. 2단계 – update\n- ’적당한 정도’를 판단하기 위한 장치: loss function 도입!\n\\[loss=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2=({\\bf y}-{\\bf\\hat{y}})^\\top({\\bf y}-{\\bf\\hat{y}})=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})\\]\n- loss 함수의 특징\n\n\\(y_i \\approx \\hat{y}_i\\) 일수록 loss값이 작다.\n\\(y_i \\approx \\hat{y}_i\\) 이 되도록 \\((\\hat{w}_0,\\hat{w}_1)\\)을 잘 찍으면 loss값이 작다.\n(중요) 주황색 점선이 ‘적당할 수록’ loss값이 작다.\n\n\nloss = torch.sum((y-yhat)**2)\nloss\n\ntensor(8587.6240, grad_fn=&lt;SumBackward0&gt;)\n\n\n- 우리의 목표: 이 loss(=8587.6240)을 더 줄이자.\n\n궁극적으로는 아예 모든 조합 \\((\\hat{w}_0,\\hat{w}_1)\\)에 대하여 가장 작은 loss를 찾으면 좋겠다.\n\n- 발상의 전환: 가만히 보니까 loss는 \\(\\hat{\\bf W} =\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\) 에 따라서 값이 바뀌는 함수잖아??? 즉 아래와 같이 생각할 수 있음.\n\\[ loss(\\hat{w}_0,\\hat{w}_1) := loss(\\hat{\\bf W})=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})\\]\n따라서 구하고 싶은것은 아래와 같음\n\\[\\hat{\\bf W} := \\underset{\\bf W}{\\operatorname{argmin}} ~ loss({\\bf W})\\]\n- \\(loss({\\bf W})\\)를 최소로 만드는 \\({\\bf W}\\)를 컴퓨터로 구하는 방법, 즉 \\(\\hat{\\bf W} := \\underset{\\bf W}{\\operatorname{argmin}} ~ loss({\\bf W})\\)를 구하는 방법을 요약하면 아래와 같다.\n1. 임의의 점 \\(\\hat{\\bf W}\\)를 찍는다.\n2. 그 점에서 순간기울기를 구한다. 즉 \\(\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})\\) 를 계산한다.\n3. \\(\\hat{\\bf W}\\)에서의 순간기울기2의 부호를 살펴보고 부호와 반대방향으로 움직인다. 이때 기울기의 절대값 크기3와 비례하여 보폭(=움직이는 정도)을 각각 조절한다. 즉 아래의 수식에 따라 업데이트 한다.\n2 \\(\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})\\)3 \\(\\left|\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})\\right|\\)\\[\\hat{\\bf W} \\leftarrow \\hat{\\bf W} - \\alpha \\times \\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})\\]\n- 여기에서 미분을 어떻게…?? 즉 아래를 어떻게 계산해..?\n\\[\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W}):= \\begin{bmatrix} \\frac{\\partial}{\\partial w_0} \\\\ \\frac{\\partial}{\\partial w_1}\\end{bmatrix}loss({\\bf W}) =  \\begin{bmatrix} \\frac{\\partial}{\\partial w_0}loss({\\bf W}) \\\\ \\frac{\\partial}{\\partial w_1}loss({\\bf W})\\end{bmatrix} \\]\n\nloss.backward()를 실행하면 What.grad에 미분값이 업데이트 되어요!\n\n(실행전)\n\nprint(What.grad)\n\nNone\n\n\n(실행후)\n\nloss.backward()\n\n\nprint(What.grad)\n\ntensor([[-1342.2465],\n        [ 1188.9203]])\n\n\n- 계산결과의 검토 (1)\n\n\\(loss({\\bf W})=({\\bf y}-\\hat{\\bf y})^\\top ({\\bf y}-\\hat{\\bf y})=({\\bf y}-{\\bf XW})^\\top ({\\bf y}-{\\bf XW})\\)\n\\(\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})=-2{\\bf X}^\\top {\\bf y}+2{\\bf X}^\\top {\\bf X W}\\)\n\n\n- 2 * X.T @ y + 2 * X.T @ X @ What\n\ntensor([[-1342.2465],\n        [ 1188.9198]], grad_fn=&lt;AddBackward0&gt;)\n\n\n- 계산결과의 검토 (2)\n\\[\\frac{\\partial}{\\partial {\\bf W} } loss({\\bf W})=\\begin{bmatrix}\\frac{\\partial}{\\partial w_0} \\\\ \\frac{\\partial}{\\partial w_1} \\end{bmatrix}loss({\\bf W}) =\\begin{bmatrix}\\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\\\ \\frac{\\partial}{\\partial w_1}loss(w_0,w_1) \\end{bmatrix}\\]\n를 계산하고 싶은데 벡터미분을 할줄 모른다고 하자. 편미분의 정의를 살펴보면,\n\\[\\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\approx \\frac{loss(w_0+h,w_1)-loss(w_0,w_1)}{h}\\]\n\\[\\frac{\\partial}{\\partial w_1}loss(w_0,w_1) \\approx \\frac{loss(w_0,w_1+h)-loss(w_0,w_1)}{h}\\]\n라고 볼 수 있다. 이를 이용하여 근사계산하면\n\ndef l(w0,w1):\n    return torch.sum((y-w0-w1*x)**2)\n\n\nl(-5,10), loss # 로스값일치\n\n(tensor(8587.6240), tensor(8587.6240, grad_fn=&lt;SumBackward0&gt;))\n\n\n\nh=0.001 \n(l(-5+h,10) - l(-5,10))/h\n\ntensor(-1342.7733)\n\n\n\nh=0.001 \n(l(-5,10+h) - l(-5,10))/h\n\ntensor(1189.4531)\n\n\n이 값은 What.grad에 저장된 값과 거의 비슷하다.\n\nWhat.grad\n\ntensor([[-1342.2465],\n        [ 1188.9203]])\n\n\n- 이제 아래의 공식에 넣고 업데이트해보자\n\\[\\hat{\\bf W} \\leftarrow \\hat{\\bf W} - \\alpha \\times \\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})\\]\n\nalpha = 0.001 \nprint(f\"{What.data} -- 수정전\")\nprint(f\"{-alpha*What.grad} -- 수정하는폭\")\nprint(f\"{What.data-alpha*What.grad} -- 수정후\")\nprint(f\"{torch.linalg.inv((X.T @ X)) @ X.T @ y} -- 회귀분석으로 구한값\")\nprint(f\"{torch.tensor([[2.5],[4]])} -- 참값(이건 비밀~~)\")\n\ntensor([[-5.],\n        [10.]]) -- 수정전\ntensor([[ 1.3422],\n        [-1.1889]]) -- 수정하는폭\ntensor([[-3.6578],\n        [ 8.8111]]) -- 수정후\ntensor([[2.4459],\n        [4.0043]]) -- 회귀분석으로 구한값\ntensor([[2.5000],\n        [4.0000]]) -- 참값(이건 비밀~~)\n\n\n\nalpha를 잘 잡아야함~\n\n- 1회 수정결과를 시각화\n\nWbefore = What.data\nWafter = What.data - alpha * What.grad \nWbefore, Wafter\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-3.6578],\n         [ 8.8111]]))\n\n\n\nplt.plot(x,y,'o',label=r'observed data')\nplt.plot(x,X@Wbefore,'--', label=r\"$\\hat{\\bf y}_{before}={\\bf X}@\\hat{\\bf W}_{before}$\")\nplt.plot(x,X@Wafter,'--', label=r\"$\\hat{\\bf y}_{after}={\\bf X}@\\hat{\\bf W}_{after}$\")\nplt.legend()"
  },
  {
    "objectID": "posts/02wk-1.html#d.-3단계-iteration-learn-estimate-bfhat-w",
    "href": "posts/02wk-1.html#d.-3단계-iteration-learn-estimate-bfhat-w",
    "title": "02wk-1: 딥러닝의 기초 (2) – Step1 ~ 4",
    "section": "D. 3단계 – iteration (=learn = estimate \\(\\bf{\\hat W}\\))",
    "text": "D. 3단계 – iteration (=learn = estimate \\(\\bf{\\hat W}\\))\n\nx = torch.tensor(temp).reshape(-1,1)\nones = torch.ones(100).reshape(-1,1)\nX = torch.concat([ones,x],axis=1)\ny = torch.tensor(sales).reshape(-1,1)\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nfor epoc in range(30):\n    yhat = X @ What\n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.001 * What.grad\n    What.grad = None\n\n\nplt.plot(x,y,'o', label = \"ovserved data\")\nplt.plot(x,X@What.data,'--', label = r\"$\\hat{\\bf y}={\\bf X}@\\hat{\\bf W}$ after 30 iterations (=epochs)\")\nplt.legend()"
  },
  {
    "objectID": "posts/02wk-1.html#a.-단순무식한-print",
    "href": "posts/02wk-1.html#a.-단순무식한-print",
    "title": "02wk-1: 딥러닝의 기초 (2) – Step1 ~ 4",
    "section": "A. 단순무식한 print",
    "text": "A. 단순무식한 print\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nalpha = 0.001\nprint(f\"시작값 = {What.data.reshape(-1)}\")\nfor epoc in range(30):\n    yhat = X @ What\n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.001 * What.grad\n    print(f'loss = {loss:.2f} \\t 업데이트폭 = {-0.001 * What.grad.reshape(-1)} \\t 업데이트결과: {What.data.reshape(-1)}')\n    What.grad = None\n\n시작값 = tensor([-5., 10.])\nloss = 8587.62   업데이트폭 = tensor([ 1.3422, -1.1889])      업데이트결과: tensor([-3.6578,  8.8111])\nloss = 5675.18   업데이트폭 = tensor([ 1.1029, -0.9499])      업데이트결과: tensor([-2.5548,  7.8612])\nloss = 3755.63   업데이트폭 = tensor([ 0.9056, -0.7596])      업데이트결과: tensor([-1.6492,  7.1016])\nloss = 2489.58   업데이트폭 = tensor([ 0.7431, -0.6081])      업데이트결과: tensor([-0.9061,  6.4935])\nloss = 1654.04   업데이트폭 = tensor([ 0.6094, -0.4872])      업데이트결과: tensor([-0.2967,  6.0063])\nloss = 1102.33   업데이트폭 = tensor([ 0.4995, -0.3907])      업데이트결과: tensor([0.2028, 5.6156])\nloss = 737.85    업데이트폭 = tensor([ 0.4091, -0.3136])      업데이트결과: tensor([0.6119, 5.3020])\nloss = 496.97    업데이트폭 = tensor([ 0.3350, -0.2519])      업데이트결과: tensor([0.9469, 5.0501])\nloss = 337.72    업데이트폭 = tensor([ 0.2742, -0.2025])      업데이트결과: tensor([1.2211, 4.8477])\nloss = 232.40    업데이트폭 = tensor([ 0.2243, -0.1629])      업데이트결과: tensor([1.4453, 4.6848])\nloss = 162.73    업데이트폭 = tensor([ 0.1834, -0.1311])      업데이트결과: tensor([1.6288, 4.5537])\nloss = 116.64    업데이트폭 = tensor([ 0.1500, -0.1056])      업데이트결과: tensor([1.7787, 4.4481])\nloss = 86.13     업데이트폭 = tensor([ 0.1226, -0.0851])      업데이트결과: tensor([1.9013, 4.3629])\nloss = 65.94     업데이트폭 = tensor([ 0.1001, -0.0687])      업데이트결과: tensor([2.0014, 4.2942])\nloss = 52.57     업데이트폭 = tensor([ 0.0818, -0.0554])      업데이트결과: tensor([2.0832, 4.2388])\nloss = 43.72     업데이트폭 = tensor([ 0.0668, -0.0447])      업데이트결과: tensor([2.1500, 4.1941])\nloss = 37.86     업데이트폭 = tensor([ 0.0545, -0.0361])      업데이트결과: tensor([2.2045, 4.1579])\nloss = 33.98     업데이트폭 = tensor([ 0.0445, -0.0292])      업데이트결과: tensor([2.2490, 4.1287])\nloss = 31.41     업데이트폭 = tensor([ 0.0363, -0.0236])      업데이트결과: tensor([2.2853, 4.1051])\nloss = 29.70     업데이트폭 = tensor([ 0.0296, -0.0191])      업데이트결과: tensor([2.3150, 4.0860])\nloss = 28.58     업데이트폭 = tensor([ 0.0242, -0.0155])      업데이트결과: tensor([2.3391, 4.0705])\nloss = 27.83     업데이트폭 = tensor([ 0.0197, -0.0125])      업데이트결과: tensor([2.3589, 4.0580])\nloss = 27.33     업데이트폭 = tensor([ 0.0161, -0.0101])      업데이트결과: tensor([2.3749, 4.0479])\nloss = 27.01     업데이트폭 = tensor([ 0.0131, -0.0082])      업데이트결과: tensor([2.3881, 4.0396])\nloss = 26.79     업데이트폭 = tensor([ 0.0107, -0.0067])      업데이트결과: tensor([2.3988, 4.0330])\nloss = 26.65     업데이트폭 = tensor([ 0.0087, -0.0054])      업데이트결과: tensor([2.4075, 4.0276])\nloss = 26.55     업데이트폭 = tensor([ 0.0071, -0.0044])      업데이트결과: tensor([2.4146, 4.0232])\nloss = 26.49     업데이트폭 = tensor([ 0.0058, -0.0035])      업데이트결과: tensor([2.4204, 4.0197])\nloss = 26.45     업데이트폭 = tensor([ 0.0047, -0.0029])      업데이트결과: tensor([2.4251, 4.0168])\nloss = 26.42     업데이트폭 = tensor([ 0.0038, -0.0023])      업데이트결과: tensor([2.4289, 4.0145])"
  },
  {
    "objectID": "posts/02wk-1.html#b.-반복시각화-yhat의-관점에서",
    "href": "posts/02wk-1.html#b.-반복시각화-yhat의-관점에서",
    "title": "02wk-1: 딥러닝의 기초 (2) – Step1 ~ 4",
    "section": "B. 반복시각화 – yhat의 관점에서!",
    "text": "B. 반복시각화 – yhat의 관점에서!\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nalpha = 0.001\nfig = plt.plot(x,y,'o',label = \"observed\")\nplt.plot(x,X@What.data,'--',color=\"C1\")\nfor epoc in range(30):\n    yhat = X @ What\n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.001 * What.grad\n    plt.plot(x,X@What.data,'--',color=\"C1\",alpha=0.1)\n    What.grad = None"
  },
  {
    "objectID": "posts/02wk-1.html#c.-반복시각화-loss의-관점에서",
    "href": "posts/02wk-1.html#c.-반복시각화-loss의-관점에서",
    "title": "02wk-1: 딥러닝의 기초 (2) – Step1 ~ 4",
    "section": "C. 반복시각화 – loss의 관점에서!!",
    "text": "C. 반복시각화 – loss의 관점에서!!\n\ndef plot_loss():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    w0 = np.arange(-6, 11, 0.5) \n    w1 = np.arange(-6, 11, 0.5)\n    W1,W0 = np.meshgrid(w1,w0)\n    LOSS=W0*0\n    for i in range(len(w0)):\n        for j in range(len(w1)):\n            LOSS[i,j]=torch.sum((y-w0[i]-w1[j]*x)**2)\n    ax.plot_surface(W0, W1, LOSS, rstride=1, cstride=1, color='b',alpha=0.1)\n    ax.azim = 30  ## 3d plot의 view 조절 \n    ax.dist = 8   ## 3d plot의 view 조절 \n    ax.elev = 5   ## 3d plot의 view 조절 \n    ax.set_xlabel(r'$w_0$')  # x축 레이블 설정\n    ax.set_ylabel(r'$w_1$')  # y축 레이블 설정\n    ax.set_xticks([-5,0,5,10])  # x축 틱 간격 설정\n    ax.set_yticks([-5,0,5,10])  # y축 틱 간격 설정\n    return fig\n\n\nl(-5,10)\n\ntensor(8587.6240)\n\n\n\nfig = plot_loss()\n\n\n\n\n\n\n\n\n\nfig = plot_loss()\nax = fig.gca()\nax.scatter(2.5, 4, l(2.5,4), s=200, marker='*', color='red', label=r\"${\\bf W}=[2.5, 4]'$\")\nax.scatter(-5, 10, l(-5,10), s=200, marker='*', color='blue', label=r\"initial $\\hat{\\bf W}=[-5, 10]'$\")\nax.legend()\n\n\n\n\n\n\n\n\n\nw0,w1 = What.data.reshape(-1)\n\n\nWhat.data\n\ntensor([[2.4289],\n        [4.0145]])\n\n\n\nw0,w1\n\n(tensor(2.4289), tensor(4.0145))\n\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nalpha = 0.001\nfor epoc in range(30):\n    yhat = X @ What\n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.001 * What.grad\n    w0,w1 = What.data.reshape(-1) \n    ax.scatter(w0,w1,l(w0,w1),s=5,marker='o',color='blue')\n    What.grad = None\n\n\nfig"
  },
  {
    "objectID": "posts/02wk-1.html#d.-애니메이션",
    "href": "posts/02wk-1.html#d.-애니메이션",
    "title": "02wk-1: 딥러닝의 기초 (2) – Step1 ~ 4",
    "section": "D. 애니메이션",
    "text": "D. 애니메이션\n\nfrom matplotlib import animation\n\n\nplt.rcParams['figure.figsize'] = (7.5,2.5)\nplt.rcParams[\"animation.html\"] = \"jshtml\" \n\n\ndef show_animation(alpha=0.001):\n    ## 1. 히스토리 기록을 위한 list 초기화\n    loss_history = [] \n    yhat_history = [] \n    What_history = [] \n\n    ## 2. 학습 + 학습과정기록\n    What= torch.tensor([[-5.0],[10.0]],requires_grad=True)\n    What_history.append(What.data.tolist())\n    for epoc in range(30): \n        yhat=X@What ; yhat_history.append(yhat.data.tolist())\n        loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n        loss.backward() \n        What.data = What.data - alpha * What.grad; What_history.append(What.data.tolist())\n        What.grad = None    \n\n    ## 3. 시각화 \n    fig = plt.figure()\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\n    #### ax1: yhat의 관점에서.. \n    ax1.plot(x,y,'o',label=r\"$(x_i,y_i)$\")\n    line, = ax1.plot(x,yhat_history[0],label=r\"$(x_i,\\hat{y}_i)$\") \n    ax1.legend()\n    #### ax2: loss의 관점에서.. \n    w0 = np.arange(-6, 11, 0.5) \n    w1 = np.arange(-6, 11, 0.5)\n    W1,W0 = np.meshgrid(w1,w0)\n    LOSS=W0*0\n    for i in range(len(w0)):\n        for j in range(len(w1)):\n            LOSS[i,j]=torch.sum((y-w0[i]-w1[j]*x)**2)\n    ax2.plot_surface(W0, W1, LOSS, rstride=1, cstride=1, color='b',alpha=0.1)\n    ax2.azim = 30  ## 3d plot의 view 조절 \n    ax2.dist = 8   ## 3d plot의 view 조절 \n    ax2.elev = 5   ## 3d plot의 view 조절 \n    ax2.set_xlabel(r'$w_0$')  # x축 레이블 설정\n    ax2.set_ylabel(r'$w_1$')  # y축 레이블 설정\n    ax2.set_xticks([-5,0,5,10])  # x축 틱 간격 설정\n    ax2.set_yticks([-5,0,5,10])  # y축 틱 간격 설정\n    ax2.scatter(2.5, 4, l(2.5,4), s=200, marker='*', color='red', label=r\"${\\bf W}=[2.5, 4]'$\")\n    ax2.scatter(-5, 10, l(-5,10), s=200, marker='*', color='blue')\n    ax2.legend()\n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        ax2.scatter(np.array(What_history)[epoc,0],np.array(What_history)[epoc,1],loss_history[epoc],color='grey')\n        fig.suptitle(f\"alpha = {alpha} / epoch = {epoc}\")\n        return line\n\n    ani = animation.FuncAnimation(fig, animate, frames=30)\n    plt.close()\n    return ani\n\n\nepoch = 0 부터 시작하여 시작점에서 출발하도록 애니메이션을 수정했습니당.\n\n\nani = show_animation(alpha=0.001)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/02wk-1.html#e.-학습률에-따른-시각화",
    "href": "posts/02wk-1.html#e.-학습률에-따른-시각화",
    "title": "02wk-1: 딥러닝의 기초 (2) – Step1 ~ 4",
    "section": "E. 학습률에 따른 시각화",
    "text": "E. 학습률에 따른 시각화\n- \\(\\alpha\\)가 너무 작다면 비효율적임\n\nshow_animation(alpha=0.0001)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- \\(\\alpha\\)가 크다고 무조건 좋은건 또 아님\n\nshow_animation(alpha=0.0083)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 수틀리면 수렴안할수도??\n\nshow_animation(alpha=0.0085)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 그냥 망할수도??\n\nshow_animation(alpha=0.01)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/01wk-1.html#a.-최하니",
    "href": "posts/01wk-1.html#a.-최하니",
    "title": "01wk-1: 이미지 자료 분석 (겉핥기)",
    "section": "A. 최하니",
    "text": "A. 최하니\n\nhani1 = PILImage.create(requests.get('https://github.com/guebin/DL2024/blob/main/imgs/01wk-hani1.jpeg?raw=true').content)\nhani1\n\n\n\n\n\n\n\n\n\nlrnr.predict(hani1)\n\n\n\n\n\n\n\n\n('dog', tensor(1), tensor([2.9308e-09, 1.0000e+00]))\n\n\n\nhani2 = PILImage.create(requests.get('https://github.com/guebin/DL2024/blob/main/imgs/01wk-hani2.jpeg?raw=true').content)\nhani2\n\n\n\n\n\n\n\n\n\nlrnr.predict(hani2)\n\n\n\n\n\n\n\n\n('dog', tensor(1), tensor([8.9153e-06, 9.9999e-01]))\n\n\n\nhani3 = PILImage.create(requests.get('https://github.com/guebin/DL2024/blob/main/imgs/01wk-hani3.jpg?raw=true').content)\nhani3\n\n\n\n\n\n\n\n\n\nlrnr.predict(hani3)\n\n\n\n\n\n\n\n\n('dog', tensor(1), tensor([3.9399e-04, 9.9961e-01]))"
  },
  {
    "objectID": "posts/01wk-1.html#b.-인터넷-고양이",
    "href": "posts/01wk-1.html#b.-인터넷-고양이",
    "title": "01wk-1: 이미지 자료 분석 (겉핥기)",
    "section": "B. 인터넷 고양이",
    "text": "B. 인터넷 고양이\n\ncat1 = PILImage.create(requests.get('https://github.com/guebin/DL2024/blob/main/imgs/01wk-cat1.png?raw=true').content)\ncat1\n\n\n\n\n\n\n\n\n\nlrnr.predict(cat1)\n\n\n\n\n\n\n\n\n('cat', tensor(0), tensor([1.0000e+00, 2.2026e-11]))\n\n\n\ncat2 = PILImage.create(requests.get('https://github.com/guebin/DL2024/blob/main/imgs/01wk-cat2.jpeg?raw=true').content)\ncat2\n\n\n\n\n\n\n\n\n\nlrnr.predict(cat2)\n\n\n\n\n\n\n\n\n('cat', tensor(0), tensor([1.0000e+00, 9.4345e-07]))"
  },
  {
    "objectID": "posts/01wk-1.html#a.-step1-dls데이터-준비",
    "href": "posts/01wk-1.html#a.-step1-dls데이터-준비",
    "title": "01wk-1: 이미지 자료 분석 (겉핥기)",
    "section": "A. Step1: DLS(=데이터) 준비",
    "text": "A. Step1: DLS(=데이터) 준비\n\ndls = ImageDataLoaders.from_folder(\n    path = './images',\n    train='train',\n    valid_pct = 0.2,\n    item_tfms=Resize(224),\n)\n\n\ndls.show_batch()"
  },
  {
    "objectID": "posts/01wk-1.html#b.-step2-러너생성",
    "href": "posts/01wk-1.html#b.-step2-러너생성",
    "title": "01wk-1: 이미지 자료 분석 (겉핥기)",
    "section": "B. Step2: 러너생성",
    "text": "B. Step2: 러너생성\n\nlrnr = vision_learner(\n    dls = dls,\n    arch = resnet34,\n    metrics = accuracy\n)"
  },
  {
    "objectID": "posts/01wk-1.html#c.-step3-학습",
    "href": "posts/01wk-1.html#c.-step3-학습",
    "title": "01wk-1: 이미지 자료 분석 (겉핥기)",
    "section": "C. Step3: 학습",
    "text": "C. Step3: 학습\n\nlrnr.fine_tune(7)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.346890\n0.969989\n0.657534\n00:11\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.734809\n0.817061\n0.698630\n00:10\n\n\n1\n0.581782\n0.937060\n0.739726\n00:11\n\n\n2\n0.426661\n0.901986\n0.835616\n00:12\n\n\n3\n0.332050\n0.899157\n0.835616\n00:10\n\n\n4\n0.263004\n0.844802\n0.849315\n00:10\n\n\n5\n0.220254\n0.762331\n0.849315\n00:11\n\n\n6\n0.185242\n0.716601\n0.849315\n00:11"
  },
  {
    "objectID": "posts/01wk-1.html#d.-step4-예측",
    "href": "posts/01wk-1.html#d.-step4-예측",
    "title": "01wk-1: 이미지 자료 분석 (겉핥기)",
    "section": "D. Step4: 예측",
    "text": "D. Step4: 예측\n\nlrnr.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninter = Interpretation.from_learner(lrnr)\ninter.plot_top_losses(16)"
  },
  {
    "objectID": "posts/01wk-1.html#크롤링을-활용한-이미지-자료-분석",
    "href": "posts/01wk-1.html#크롤링을-활용한-이미지-자료-분석",
    "title": "01wk-1: 이미지 자료 분석 (겉핥기)",
    "section": "#. 크롤링을 활용한 이미지 자료 분석",
    "text": "#. 크롤링을 활용한 이미지 자료 분석\n(1) 두 가지 키워드로 크롤링을 수행하여 이미지자료를 모아라. (키워드는 각자 마음에 드는 것으로 설정할 것, 단 (iu,hynn)는 제외)\n(2) ImageDataLoaders.from_folder() 를 이용하여 dls를 만들고 dls.show_batch()를 이용하여 만들어진 이미지를 확인하라.\n(3) vision_learner()를 이용하여 lrnr를 만들고 lrnr.fine_tune()을 이용하여 학습하라. 이때 모형의 arch는 resnet34를 사용하라.\n(4) requests.get()을 이용하여 (1)의 키워드에 해당하는 새로운 이미지를 한장씩 다운받고 (3)에서 학습한 lrnr를 이용하여 예측하라.\n\n제출은 ipynb파일로 할 것. 혹은 스크린샷을 제출해도 괜찮음."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/02wk-2.html#a.-data",
    "href": "posts/02wk-2.html#a.-data",
    "title": "02wk-2: 딥러닝의 기초 (3) – Step1,2,4 의 변형",
    "section": "A. Data",
    "text": "A. Data\n\ntemp = [-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632]\nsales= [-8.5420, -6.5767, -5.9496, -4.4794, -4.2516, -3.1326, -4.0239, -4.1862,\n        -3.3403, -2.2027, -2.0262, -2.5619, -1.3353, -2.0466, -0.4664, -1.3513,\n        -1.6472, -0.1089, -0.3071, -0.6299, -0.0438,  0.4163,  0.4166, -0.0943,\n         0.2662,  0.4591,  0.8905,  0.8998,  0.6314,  1.3845,  0.8085,  1.2594,\n         1.1211,  1.9232,  1.0619,  1.3552,  2.1161,  1.1437,  1.6245,  1.7639,\n         1.6022,  1.7465,  0.9830,  1.7824,  2.1116,  2.8621,  2.1165,  1.5226,\n         2.5572,  2.8361,  3.3956,  2.0679,  2.8140,  3.4852,  3.6059,  2.5966,\n         2.8854,  3.9173,  3.6527,  4.1029,  4.3125,  3.4026,  3.2180,  4.5686,\n         4.3772,  4.3075,  4.4895,  4.4827,  5.3170,  5.4987,  5.4632,  6.0328,\n         5.2842,  5.0539,  5.4538,  6.0337,  5.7250,  5.7587,  6.2020,  6.5992,\n         6.4621,  6.5140,  6.6846,  7.3497,  8.0909,  7.0794,  6.8667,  7.4229,\n         7.2544,  7.1967,  9.5006,  9.0339,  7.4887,  9.0759, 11.0946, 10.3260,\n        12.2665, 13.0983, 12.5468, 13.8340]\nx = torch.tensor(temp).reshape(-1,1)\nones = torch.ones(100).reshape(-1,1)\nX = torch.concat([ones,x],axis=1)\ny = torch.tensor(sales).reshape(-1,1)"
  },
  {
    "objectID": "posts/02wk-2.html#b.-파이토치를-이용한-학습",
    "href": "posts/02wk-2.html#b.-파이토치를-이용한-학습",
    "title": "02wk-2: 딥러닝의 기초 (3) – Step1,2,4 의 변형",
    "section": "B. 파이토치를 이용한 학습",
    "text": "B. 파이토치를 이용한 학습\n- 외우세여\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad = True)\nfor epoc in range(30):\n    # step1: yhat \n    yhat = X@What \n    # step2: loss\n    loss = torch.sum((y-yhat)**2)\n    # step3: 미분\n    loss.backward()\n    # step4: update\n    What.data = What.data - 0.001 * What.grad\n    What.grad = None\n\n- 결과 시각화\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What.data,'--')\nplt.title(f'What={What.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/02wk-2.html#c.-step2의-수정",
    "href": "posts/02wk-2.html#c.-step2의-수정",
    "title": "02wk-2: 딥러닝의 기초 (3) – Step1,2,4 의 변형",
    "section": "C. Step2의 수정",
    "text": "C. Step2의 수정\n- 수정된 코드\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad = True)\nloss_fn = torch.nn.MSELoss()\nfor epoc in range(30):\n    # step1: yhat \n    yhat = X@What \n    # step2: loss\n    #loss = torch.sum((y-yhat)**2)/100\n    #loss = torch.mean((y-yhat)**2) \n    loss = loss_fn(yhat,y) # 여기서는 큰 상관없지만 습관적으로 yhat을 먼저넣는 연습을 하자!!\n    # step3: 미분\n    loss.backward()\n    # step4: update\n    What.data = What.data - 0.1 * What.grad\n    What.grad = None\n\n- 결과확인\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What.data,'--')\nplt.title(f'What={What.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/02wk-2.html#d.-step1의-수정-net의-이용",
    "href": "posts/02wk-2.html#d.-step1의-수정-net의-이용",
    "title": "02wk-2: 딥러닝의 기초 (3) – Step1,2,4 의 변형",
    "section": "D. Step1의 수정 – net의 이용",
    "text": "D. Step1의 수정 – net의 이용\n- net 오브젝트란?\n원래 yhat을 이런식으로 구했는데 ~\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad = True)\n(X@What.data)[:5]\n\ntensor([[-29.8210],\n        [-28.6210],\n        [-24.9730],\n        [-21.2390],\n        [-19.7920]])\n\n\n이런식으로도 구할수 있음!\n\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\n\n\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\nnet.weight\n\nParameter containing:\ntensor([[-5., 10.]], requires_grad=True)\n\n\n\nnet(X)[:5]\n\ntensor([[-29.8210],\n        [-28.6210],\n        [-24.9730],\n        [-21.2390],\n        [-19.7920]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n- 학습\n\n# step1을 위한 사전준비\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2를 위한 사전준비\nloss_fn = torch.nn.MSELoss()\nfor epoc in range(30):\n    # step1: yhat \n    yhat = net(X)\n    # step2: loss\n    loss = loss_fn(yhat,y)\n    # step3: 미분\n    loss.backward()\n    # step4: update\n    net.weight.data = net.weight.data - 0.1 * net.weight.grad\n    net.weight.grad = None\n\n- 결과확인\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')\nplt.title(f'net.weight={net.weight.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/02wk-2.html#e.-step4의-수정-optimizer의-이용",
    "href": "posts/02wk-2.html#e.-step4의-수정-optimizer의-이용",
    "title": "02wk-2: 딥러닝의 기초 (3) – Step1,2,4 의 변형",
    "section": "E. Step4의 수정 – optimizer의 이용",
    "text": "E. Step4의 수정 – optimizer의 이용\n기존코드의 에폭별분해\n- 준비과정\n\n# step1을 위한 사전준비\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2를 위한 사전준비\nloss_fn = torch.nn.MSELoss()\n\n- 에폭별분해\n(미분전) – step1~2 완료\n\nyhat = net(X)\nloss = loss_fn(yhat,y)\n\n\nprint(f'파라메터 = {net.weight.data}')\nprint(f'미분값 = {net.weight.grad}')\n\n파라메터 = tensor([[-5., 10.]])\n미분값 = None\n\n\n(미분후, 업데이트 진행전) – step3 완료\n\nloss.backward()\n\n\nprint(f'파라메터 = {net.weight.data}')\nprint(f'미분값 = {net.weight.grad}')\n\n파라메터 = tensor([[-5., 10.]])\n미분값 = tensor([[-13.4225,  11.8892]])\n\n\n(업데이트 진행후) – step4 의 첫째줄 완료\n\nnet.weight.data = net.weight.data - 0.1 * net.weight.grad\n\n\nprint(f'파라메터 = {net.weight.data}')\nprint(f'미분값 = {net.weight.grad}')\n\n파라메터 = tensor([[-3.6578,  8.8111]])\n미분값 = tensor([[-13.4225,  11.8892]])\n\n\n(업데이트 완료 후 초기화까지 끝냄) – step4 의 두번째줄 완료\n\nnet.weight.grad = None\n\n\nprint(f'파라메터 = {net.weight.data}')\nprint(f'미분값 = {net.weight.grad}')\n\n파라메터 = tensor([[-3.6578,  8.8111]])\n미분값 = None\n\n\n새로운코드의 에폭별분해\n- 준비과정 – 옵티마이저라는 오브젝트를 셋팅한다!\n\n# step1을 위한 사전준비\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2를 위한 사전준비\nloss_fn = torch.nn.MSELoss()\n# step3을 위한 사전준비 \noptimizr = torch.optim.SGD(params=net.parameters(),lr=0.1)\n\n- 에폭별분해\n(미분전) – step1~2 완료\n\nyhat = net(X)\nloss = loss_fn(yhat,y)\n\n\nprint(f'파라메터 = {net.weight.data}')\nprint(f'미분값 = {net.weight.grad}')\n\n파라메터 = tensor([[-5., 10.]])\n미분값 = None\n\n\n(미분후, 업데이트 진행전) – step3 완료\n\nloss.backward()\n\n\nprint(f'파라메터 = {net.weight.data}')\nprint(f'미분값 = {net.weight.grad}')\n\n파라메터 = tensor([[-5., 10.]])\n미분값 = tensor([[-13.4225,  11.8892]])\n\n\n(업데이트 진행후) – step4 의 첫째줄 완료\n\n#net.weight.data = net.weight.data - 0.1 * net.weight.grad\noptimizr.step()\n\n\nprint(f'파라메터 = {net.weight.data}')\nprint(f'미분값 = {net.weight.grad}')\n\n파라메터 = tensor([[-3.6578,  8.8111]])\n미분값 = tensor([[-13.4225,  11.8892]])\n\n\n(업데이트 완료 후 초기화까지 끝냄) – step4 의 두번째줄 완료\n\n#net.weight.grad = None\noptimizr.zero_grad()\n\n\nprint(f'파라메터 = {net.weight.data}')\nprint(f'미분값 = {net.weight.grad}')\n\n파라메터 = tensor([[-3.6578,  8.8111]])\n미분값 = None\n\n\n최종코드\n- 학습\n\n# step1을 위한 사전준비\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n# step2를 위한 사전준비\nloss_fn = torch.nn.MSELoss()\n# step4를 위한 사전준비 \noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\nfor epoc in range(30):\n    # step1: yhat \n    yhat = net(X)\n    # step2: loss\n    loss = loss_fn(yhat,y)\n    # step3: 미분\n    loss.backward()\n    # step4: update\n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과확인\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.data,'--')\nplt.title(f'net.weight={net.weight.data.reshape(-1)}');"
  },
  {
    "objectID": "posts/01wk-2.html#a.-모형소개",
    "href": "posts/01wk-2.html#a.-모형소개",
    "title": "01wk-2: 딥러닝의 기초 (1) – 회귀모형, 경사하강법",
    "section": "A. 모형소개",
    "text": "A. 모형소개\n- model: \\(y_i= w_0+w_1 x_i +\\epsilon_i = 2.5 + 4x_i +\\epsilon_i, \\quad i=1,2,\\dots,n\\)\n- model: \\({\\bf y}={\\bf X}{\\bf W} +\\boldsymbol{\\epsilon}\\)\n\n\\({\\bf y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_n\\end{bmatrix}, \\quad {\\bf X}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots \\\\ 1 & x_n\\end{bmatrix}, \\quad {\\bf W}=\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}, \\quad \\boldsymbol{\\epsilon}= \\begin{bmatrix} \\epsilon_1 \\\\ \\dots \\\\ \\epsilon_n\\end{bmatrix}\\)"
  },
  {
    "objectID": "posts/01wk-2.html#b.-회귀모형에서-데이터-생성",
    "href": "posts/01wk-2.html#b.-회귀모형에서-데이터-생성",
    "title": "01wk-2: 딥러닝의 기초 (1) – 회귀모형, 경사하강법",
    "section": "B. 회귀모형에서 데이터 생성",
    "text": "B. 회귀모형에서 데이터 생성\n\ntorch.manual_seed(43052)\nones= torch.ones(100).reshape(-1,1)\nx,_ = torch.randn(100).sort()\nx = x.reshape(-1,1)\nX = torch.concat([ones,x],axis=-1)\nW = torch.tensor([[2.5],[4]])\nϵ = torch.randn(100).reshape(-1,1)*0.5\ny = X@W + ϵ\n\n\nplt.plot(x,y,'o')\nplt.plot(x,2.5+4*x,'--')"
  },
  {
    "objectID": "posts/01wk-2.html#a.-손실함수",
    "href": "posts/01wk-2.html#a.-손실함수",
    "title": "01wk-2: 딥러닝의 기초 (1) – 회귀모형, 경사하강법",
    "section": "A. 손실함수",
    "text": "A. 손실함수\n- ’적당한 정도’를 판단하기 위한 장치: loss function 도입!\n\\(loss=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2\\)\n\\(=({\\bf y}-{\\bf\\hat{y}})^\\top({\\bf y}-{\\bf\\hat{y}})=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})\\)\n- loss 함수의 특징\n\n\\(y_i \\approx \\hat{y}_i\\) 일수록 loss값이 작다.\n\\(y_i \\approx \\hat{y}_i\\) 이 되도록 \\((\\hat{w}_0,\\hat{w}_1)\\)을 잘 찍으면 loss값이 작다.\n(중요) 주황색 점선이 ‘적당할 수록’ loss값이 작다.\n\n\nloss = torch.sum((y-yhat)**2)\nloss\n\ntensor(8587.6875, grad_fn=&lt;SumBackward0&gt;)\n\n\n- 우리의 목표: 이 loss(=8587.6875)을 더 줄이자.\n\n궁극적으로는 아예 모든 조합 \\((\\hat{w}_0,\\hat{w}_1)\\)에 대하여 가장 작은 loss를 찾으면 좋겠다. (단계2에서 할일은 아님)\n\n- 문제의 치환: 생각해보니까 우리의 문제는 아래와 같이 수학적으로 단순화 되었다.\n\n적당해보이는 주황색 선을 찾자 \\(\\to\\) \\(loss(w_0,w_1)\\)를 최소로하는 \\((w_0,w_1)\\)의 값을 찾자.\n\n- 수정된 목표: \\(loss(w_0,w_1)\\)를 최소로 하는 \\((w_0,w_1)\\)을 구하라.\n\n단순한 수학문제가 되었다. 이것은 마치 \\(f(x,y)\\)를 최소화하는 \\((x,y)\\)를 찾으라는 것임.\n함수의 최대값 혹은 최소값을 컴퓨터를 이용하여 찾는것을 “최적화”라고 하며 이는 산공교수님들이 가장 잘하는 분야임. (산공교수님들에게 부탁하면 잘해줌, 산공교수님들은 보통 최적화해서 어디에 쓸지보다 최적화 자체에 더 관심을 가지고 연구하심)\n최적화를 하는 방법? 경사하강법"
  },
  {
    "objectID": "posts/01wk-2.html#b.-경사하강법",
    "href": "posts/01wk-2.html#b.-경사하강법",
    "title": "01wk-2: 딥러닝의 기초 (1) – 회귀모형, 경사하강법",
    "section": "B. 경사하강법",
    "text": "B. 경사하강법\n- 경사하강법 아이디어 (1차원)\n\n임의의 점을 찍는다.\n그 점에서 순간기울기를 구한다. (접선) &lt;– 미분\n순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 움직인다.\n\n\n팁: 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 조절한다. \\(\\to\\) \\(\\alpha\\)를 도입\n\n\n최종수식: \\(w \\leftarrow w - \\alpha \\times \\frac{\\partial}{\\partial w}loss(w)\\)\n\n- 경사하강법 아이디어 (2차원)\n\n임의의 점을 찍는다.\n그 점에서 순간기울기를 구한다. (접평면) &lt;– 편미분\n순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 각각 움직인다.\n\n\n팁: 여기서도 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 각각 조절한다. \\(\\to\\) \\(\\alpha\\)를 도입.\n\n- 경사하강법 = loss를 줄이도록 \\({\\bf W}\\)를 개선하는 방법\n\n업데이트 공식: 수정값 = 원래값 - \\(\\alpha\\) \\(\\times\\) 기울어진크기(=미분계수)\n여기에서 \\(\\alpha\\)는 전체적인 보폭의 크기를 결정한다. 즉 \\(\\alpha\\)값이 클수록 한번의 update에 움직이는 양이 크다."
  }
]